# -*- coding: utf-8 -*-
"""YourXasY_AutoShortsGenerator.ipynb (1).txt

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BEwhdgCBvyeb0GovPHRXkZFHEUf9yA0F
"""

import locale
locale.getpreferredencoding = lambda: "UTF-8"

# prompt: mount drive
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

!pip install diffusers transformers accelerate safetensors
!pip install git+https://github.com/huggingface/parler-tts.git

import os
from diffusers import DiffusionPipeline
import torch
from parler_tts import ParlerTTSForConditionalGeneration
from transformers import AutoTokenizer
import soundfile as sf

video_title = "Anime character tournament"
responses = ["January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December"]
responses = [
    "Goku (Dragon Ball)",
    "Naruto Uzumaki (Naruto)",
    "Monkey D. Luffy (One Piece)",
    "Saitama (One Punch Man)",
    "Ichigo Kurosaki (Bleach)",
    "Spike Spiegel (Cowboy Bebop)",
    "Edward Elric (Fullmetal Alchemist)",
    "Light Yagami (Death Note)",
    "Levi Ackerman (Attack on Titan)",
    "Kagome Higurashi (Inuyasha)",
    "Asuka Langley Soryu (Neon Genesis Evangelion)",
    "Lelouch vi Britannia (Code Geass)",
    "Comment your favourite character.",
    "The most popular wins the tournament."


]
title_image_prompt = "war between anime characters , goku , luffy, zoro, naruto, sasuke, levi ackerman, saitama,masterpiece"

basic_prompt = ",Ninja, cool outfit, weapons , digital art, Catchy, cover picture"

use_basic_prompt = False

if use_basic_prompt:
  responses_image_prompts = [i+basic_prompt for i in responses]
else:
  responses_image_prompts = [
    "Goku (Dragon Ball),masterpiece,badass,strong, cool",
    "Naruto Uzumaki (Naruto),masterpiece,badass,strong, cool",
    "Monkey D. Luffy (One Piece),masterpiece,badass,strong, cool",
    "Saitama (One Punch Man),masterpiece,badass,strong, cool",
    "Ichigo Kurosaki (Bleach),masterpiece,badass,strong, cool",
    "Spike Spiegel (Cowboy Bebop),masterpiece,badass,strong, cool",
    "Edward Elric (Fullmetal Alchemist),masterpiece,badass,strong, cool",
    "Light Yagami (Death Note),masterpiece,badass,strong, cool",
    "Levi Ackerman (Attack on Titan),masterpiece,badass,strong, cool",
    "Kagome Higurashi (Inuyasha),masterpiece,badass,strong, cool",
    "Asuka Langley Soryu (Neon Genesis Evangelion),masterpiece,badass,strong, cool",
    "Lelouch vi Britannia (Code Geass),masterpiece,badass,strong, cool",
    "war between anime characters , goku , luffy, zoro, naruto, sasuke, levi ackerman, saitama,masterpiece,badass,strong, cool"
]
num_models = 2
model_save_paths = []

# prompt: remove all the file that ends with ".png" at "/content"

import glob

# Get all the files that end with ".png" at "/content"
png_files = glob.glob('/content/*.png')

# Remove all the files
for file in png_files:
  !rm -f {file}

import torch

device = "cuda" if torch.cuda.is_available() else "cpu"


model_id = "playgroundai/playground-v2.5-1024px-aesthetic"
diffusionPipe = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16,variant="fp16").to("cuda")

"""###extra stuff

"""

#torch.cuda.empty_cache()

save_path="/content/sample_data"
model_save_paths.append(save_path)

title_img =diffusionPipe(prompt=title_image_prompt,height = 960, width = 544).images[0]
title_img.save(save_path+"0_0.png")

for i,response in enumerate(responses_image_prompts):
    print(response)
    response_img = diffusionPipe(prompt=response,height = 960,width = 544).images[0]

    response_img.save(save_path+"0_"+f"{i+1}.png")

del diffusionPipe
torch.cuda.empty_cache()

replace_im = True

prmt = " war between anime characters , goku , luffy, zoro, naruto, sasuke, levi ackerman, saitama,masterpiece,badass,strong, cool"
img = "14.png"
if replace_im:
  diffusionPipe = DiffusionPipeline.from_pretrained(
    "cagliostrolab/animagine-xl-3.1",
    torch_dtype=torch.float16,
    use_safetensors=True,
).to("cuda")

  title_img =diffusionPipe(prompt=prmt).images[0]
  title_img.save(save_path+img)
  display(title_img)

from diffusers import StableDiffusionPipeline
import torch


model_id = "dreamlike-art/dreamlike-photoreal-2.0"
diffusionPipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)
diffusionPipe = diffusionPipe.to("cuda")




title_img =diffusionPipe(prompt=title_image_prompt,height = 960, width = 544).images[0]
title_img.save(save_path+"1_0.png")

for i,response in enumerate(responses_image_prompts):
    print(response)
    response_img = diffusionPipe(prompt=response,height = 960,width = 544).images[0]

    response_img.save(save_path+"1_"+f"{i+1}.png")

del diffusionPipe
torch.cuda.empty_cache()

diffusionPipe = DiffusionPipeline.from_pretrained(
    "cagliostrolab/animagine-xl-3.1",
    torch_dtype=torch.float16,
    use_safetensors=True,
).to("cuda")
title_img =diffusionPipe(prompt=title_image_prompt,height = 960, width = 544).images[0]
title_img.save(save_path+"0.png")

for i,response in enumerate(responses_image_prompts):
    print(response)
    response_img = diffusionPipe(prompt=response,height = 960,width = 544).images[0]

    response_img.save(save_path+f"{i+1}.png")

del diffusionPipe

import cv2
from google.colab.patches import cv2_imshow
import os
num_models = 2
for i in range(len(responses)+1):
  tims = []
  for x in range(num_models):
    print(f"{x}_{i}.png")
    im =cv2.imread(save_path+f"{x}_{i}.png")
    tims.append(im)
    cv2_imshow(im)
  ip  = int(input(f"select between 1 and {num_models}"))
  os.rename(save_path+f"{ip-1}_{i}.png",save_path+f"{i}.png")

import cv2
import numpy as np


if responses[0] != video_title:
    responses.insert(0,video_title)

def align_text_to_image(text, image, font, font_scale, color, thickness, alignment = "Center",X_pos = None,Y_pos = None):
  if alignment == "Center":
    text_size, _ = cv2.getTextSize(text, font, font_scale, thickness)
    if X_pos == None:
      text_x = (image.shape[1] - text_size[0]) // 2
    if Y_pos == None:
      text_y = (image.shape[0] + text_size[1]) // 2
    else:
      text_y = Y_pos

    cv2.putText(image, text, (text_x, text_y), font, font_scale, color, thickness)

def add_text_to_image(image, text, position=(330, 800), font=cv2.FONT_HERSHEY_COMPLEX, font_scale=3, color=(255, 255, 255), thickness=5):
  cv2.putText(image, text, position, font, font_scale, color, thickness)

def calculate_fps(text,vid_length):
  frames = len(text.split(" "))
  fps = frames / vid_length
  return fps

def textOnImg_vid(text,img,name="gg",frame_shape = None,title=None,title_align = "Center",font = cv2.FONT_HERSHEY_COMPLEX,vid_len=None,audio=None,fps=None,font_scale = 3):

  video_name = f'{name}.mp4'
  if fps == None:
    if vid_len ==None:
      fps=24
    else:
      fps = calculate_fps(text,vid_len)
  frame = img
  if frame_shape == None:
    frame_shape = frame.shape
  height, width, layers = frame_shape
  img = cv2.resize(img, (width, height))


  if title !=None:
    if title_align == "Center":
      textsize = cv2.getTextSize(title, font, 1, 2)[0]
      textX = (width - textsize[0]) / 2
      textY = 150
      cv2.putText(img, title, (int(textX), int(textY)), font, 1, (0, 0, 0), 8)
      cv2.putText(img, title, (int(textX), int(textY)), font, 1, (255, 255, 255), 2)

  color = (255, 255, 255)
  thickness = 2
  font = cv2.FONT_HERSHEY_COMPLEX

  #video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))


  if cv2.getTextSize(text, font, font_scale, thickness)[0][0] > width:
    words = text.split()
    lines = []
    current_line = words[0] + " "  # Start with the first word
    for word in words[1:]:
      test_line = current_line + word + " "
      test_size, _ = cv2.getTextSize(test_line, font, font_scale, thickness)
      if test_size[0] > width:
        lines.append(current_line)
        current_line = word + " "
      else:
        current_line = test_line
        # Add the last line
    lines.append(current_line)
    yp = 900
    c=0
    for line in lines:


      align_text_to_image(line, img, font, font_scale, (0,0,0), thickness*3, alignment = "Center",X_pos = None,Y_pos = yp+c*100)
      align_text_to_image(line, img, font, font_scale, color, thickness, alignment = "Center",X_pos = None,Y_pos = yp+c*100)
      c+=1
  else:
    align_text_to_image(text, img, font, font_scale, (0,0,0), thickness*3, alignment = "Center",X_pos = None,Y_pos = None)
    align_text_to_image(text, img, font, font_scale, color, thickness, alignment = "Center",X_pos = None,Y_pos = None)




  # fps = tot_frames / vid_length
  # print("TOTAL FRAMES : ",tot_frames)
  # print("FPS : ",fps)
  # print("VIDEO LENGTH : ",vid_length)
  video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))
  video.write(img)

  # Release the video writer
  video.release()
  cv2.destroyAllWindows()
  video1 = mp.VideoFileClip(video_name)
  if audio != None:
    video1 = video1.set_audio(audio)
  video1.write_videofile(f"{name}_final.mp4")



#textOnImg_vid(text,img,title="Why does it feel so good to scratch an itch?",title_align="Center",frame_shape=(1920,1080,3))
final_video = []
for i in range(len(responses)):
  import moviepy.editor as mp
  #audio = mp.AudioFileClip(f"{i}.wav")
  #vid_length = audio.duration
  img = cv2.imread(f"{i}.png")
  textOnImg_vid(responses[i],img,name=f"{i}",title=responses[0],title_align="Center",frame_shape=(1920,1080,3),fps=1)
  final_video.append(mp.VideoFileClip(f"{i}_final.mp4"))

final_video = mp.concatenate_videoclips(final_video)
music = None
if music != None:
  final_video = final_video.set_audio(mp.AudioFileClip(music).set_duration(final_video.duration))
final_video.write_videofile("final_video.mp4")